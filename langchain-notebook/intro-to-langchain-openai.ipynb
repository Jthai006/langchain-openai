{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain and OpenAI Integration Notebook\n",
    "\n",
    "This notebook demonstrates how to use LangChain with OpenAI for various NLP tasks. We'll walk through the installation, setup, and usage of different LangChain components step-by-step.\n",
    "\n",
    "## Installation\n",
    "\n",
    "First, we need to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Next, we set up environment variables necessary for LangChain and OpenAI API access.\n",
    "\n",
    "### Example .env\n",
    "\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "\n",
    "LANGCHAIN_API_KEY={{LANGCHAIN_API_KEY}}\n",
    "\n",
    "OPENAI_API_KEY={{OPENAI_API_KEY}}\n",
    "\n",
    "TAVILY_API_KEY={{TAVILY_API_KEY}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain and OpenAI Setup\n",
    "\n",
    "Import and initialize the ChatOpenAI model from LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LangSmith\n",
    "\n",
    "Invoke the model to see how LangSmith can assist with testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': 'application/json', 'Content-Type': 'application/json', 'User-Agent': 'OpenAI/Python 1.31.0', 'X-Stainless-Lang': 'python', 'X-Stainless-Package-Version': '1.31.0', 'X-Stainless-OS': 'MacOS', 'X-Stainless-Arch': 'x64', 'X-Stainless-Runtime': 'CPython', 'X-Stainless-Runtime-Version': '3.12.3', 'Authorization': 'Bearer sk-proj-piS1lV36b8mMtvTFzoBeT3BlbkFJ9Afr0m9XpiAOI1yoqI81', 'X-Stainless-Async': 'false'}\n",
      "Bearer sk-proj-piS1lV36b8mMtvTFzoBeT3BlbkFJ9Afr0m9XpiAOI1yoqI81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith can help with testing in the following ways:\\n\\n1. Language Support: Langsmith can provide language-specific libraries and tools for automated testing, making it easier to write and run tests in different programming languages.\\n\\n2. Test Automation: Langsmith can automate the process of running tests, making it easier to test code changes and updates quickly and efficiently.\\n\\n3. Test Reporting: Langsmith can generate detailed reports on test results, helping developers identify and fix bugs more effectively.\\n\\n4. Integration Testing: Langsmith can help with integration testing by providing tools and frameworks for testing how different components of a system work together.\\n\\n5. Performance Testing: Langsmith can help with performance testing by providing tools and libraries for measuring and analyzing the performance of code and systems.\\n\\nOverall, Langsmith can streamline the testing process and provide developers with the tools they need to ensure their code is functioning correctly and efficiently.', response_metadata={'token_usage': {'completion_tokens': 177, 'prompt_tokens': 15, 'total_tokens': 192}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7c6a0d09-ffe3-4c4f-976e-9dfc9e85e570-0', usage_metadata={'input_tokens': 15, 'output_tokens': 177, 'total_tokens': 192})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Chat Prompt Template\n",
    "\n",
    "Use `ChatPromptTemplate` to define a prompt for our LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer. Keep responses short and concise\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Prompt and Model\n",
    "\n",
    "Create a chain that combines the prompt and the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the Chain\n",
    "\n",
    "Invoke the chain to see how LangSmith can assist with testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Output\n",
    "\n",
    "Use `StrOutputParser` to parse the output of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the Chain with Output Parser\n",
    "\n",
    "Invoke the chain with the output parser to get a structured response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "\n",
    "Load documents from a webpage using `WebBaseLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Generate embeddings for the documents using OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitting\n",
    "\n",
    "### Recursive Character Text Splitter\n",
    "\n",
    "The `RecursiveCharacterTextSplitter` splits documents based on character count while maintaining coherence in the chunks. This is useful for processing large texts in manageable pieces.\n",
    "\n",
    "### How `split_documents` Works\n",
    "\n",
    "1. **Initialization**:\n",
    "   - The `RecursiveCharacterTextSplitter` is initialized with parameters such as `chunk_size` and `chunk_overlap`. These parameters determine the maximum size of each chunk and the amount of overlap between consecutive chunks, respectively.\n",
    "\n",
    "2. **Splitting Process**:\n",
    "   - **Character Limit**: The text is split into chunks based on a specified character limit (`chunk_size`). This ensures that each chunk does not exceed a certain length, which can be beneficial for processing in NLP models that have input size limitations.\n",
    "   - **Recursive Splitting**: If a segment of text still exceeds the `chunk_size`, the splitter recursively breaks it down further until each chunk is within the desired size.\n",
    "   - **Overlap**: To maintain context between chunks, a certain number of characters from the end of one chunk are included at the beginning of the next chunk. This overlap (`chunk_overlap`) helps in preserving the continuity of the text across chunks.\n",
    "\n",
    "3. **Final Output**:\n",
    "   - The method returns a list of smaller documents or text chunks. Each chunk is a substring of the original document, and these chunks are designed to be contextually coherent and manageable in size.\n",
    "\n",
    "### Example of How It Works\n",
    "\n",
    "Here's a simple conceptual example:\n",
    "\n",
    "Suppose you have a document that is a long paragraph of 1000 characters, and you set `chunk_size` to 200 and `chunk_overlap` to 50.\n",
    "\n",
    "- **First Chunk**: Characters 1 to 200.\n",
    "- **Second Chunk**: Characters 151 to 350 (including 50 characters overlap with the first chunk).\n",
    "- **Third Chunk**: Characters 301 to 500, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation\n",
    "\n",
    "### Process Overview\n",
    "\n",
    "1. **Query the Retriever**: The input question is passed to the retriever to find relevant documents.\n",
    "2. **Combine and Process Documents**: The retrieved documents are combined into a context for the LLM.\n",
    "3. **Generate Response with LLM**: The LLM generates a response based on the context and the question.\n",
    "\n",
    "### Benefits of This Approach\n",
    "\n",
    "- **Relevance**: The retriever ensures that the documents provided to the LLM are highly relevant to the query, improving the quality of the generated response.\n",
    "- **Efficiency**: By retrieving only the most relevant documents, the system can efficiently handle large document collections.\n",
    "- **Contextual Accuracy**: The combined approach allows the LLM to generate responses that are more accurate and contextually appropriate, as it has access to pertinent information retrieved by the retriever.\n",
    "\n",
    "This setup effectively leverages the strengths of both the retriever and the LLM, providing a powerful solution for generating detailed and context-aware responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the Retrieval Chain\n",
    "\n",
    "Invoke the retrieval chain to answer the question based on the context from the retrieved documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History-Aware Retriever\n",
    "\n",
    "Use history-aware retrieval to incorporate conversation history into the context for more accurate and relevant responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining History and Context\n",
    "\n",
    "Combine the chat history and document context to generate more relevant responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Retriever Tool\n",
    "\n",
    "Create a tool for retrieving information about LangSmith.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Tools\n",
    "\n",
    "Integrate additional tools such as Tavily Search for broader information retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()\n",
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an OpenAI Functions Agent\n",
    "\n",
    "Create an agent that uses OpenAI functions and integrates the defined tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# You need to set OPENAI_API_KEY environment variable or pass it as argument `api_key`.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the Agent\n",
    "\n",
    "Invoke the agent to see how LangSmith can help with testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Queries\n",
    "\n",
    "Invoke the agent with additional queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"what is the weather in SF?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History-Aware Agent\n",
    "\n",
    "Use the history-aware agent for conversation-based queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "agent_executor.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
